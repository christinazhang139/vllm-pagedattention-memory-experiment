# vLLM PagedAttention Memory Analysis Experiment

> ğŸ§ª A hands-on experiment to understand PagedAttention's memory efficiency advantages in large language model inference

## ğŸ“‹ Project Information

**Repository Name:** `vllm-paged-attention-memory-experiment`

**Project Description:** Deep understanding of memory-efficient large language model inference by comparing traditional Transformers with vLLM's PagedAttention

**Tags:** `llm`, `paged-attention`, `vllm`, `transformers`, `memory-optimization`, `gpu`, `inference`, `experiment`, `pytorch`

## ğŸ¯ Project Overview

This project provides a practical, hands-on experiment to understand **PagedAttention** - the core innovation behind vLLM's memory-efficient large language model inference. Through direct comparison between traditional Transformers and vLLM methods, you will observe:

- ğŸ’¾ **Memory allocation patterns**
- ğŸ“Š **Memory usage stability** 
- âš¡ **Batch processing efficiency**
- ğŸ”¥ **GPU utilization optimization**

## ğŸ”¬ What You'll Learn

### Problems with Traditional Approaches:
- Sequential request processing
- Memory allocation by maximum possible length
- Memory fragmentation and waste
- Poor GPU utilization

### PagedAttention Solutions:
- **Paged KV-Cache Management**: Split attention cache into fixed-size pages
- **Dynamic Memory Allocation**: Allocate pages on-demand
- **Batch Processing**: Efficiently handle variable-length sequences
- **Reduced Memory Fragmentation**: Better memory utilization

## ğŸš€ Quick Start

### System Requirements

- **GPU**: NVIDIA GPU with CUDA support (tested on RTX 4090)
- **Python**: 3.9+ (tested on 3.12.3)
- **CUDA**: 12.0+ recommended
- **VRAM**: 16GB+ recommended

### Installation Steps

1. **Create directory**
   ```bash
   cd paged-attention-memory-experiment
   ```

2. **Create virtual environment**
   ```bash
   python3 -m venv venv-paged-test
   source venv-paged-test/bin/activate  # Linux/Mac
   # or
   venv-paged-test\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install --upgrade pip
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   pip install transformers>=4.36.0
   pip install vllm
   pip install accelerate
   pip install nvidia-ml-py3 psutil
   ```

4. **Run experiment**
   ```bash
   python run_experiment.py
   ```

## ğŸ“ Project Structure

```
paged-attention-memory-experiment/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ memory_monitor.py      # GPU memory monitoring utilities
â”‚   â”œâ”€â”€ test_traditional.py    # Traditional Transformers test
â”‚   â””â”€â”€ test_vllm.py          # vLLM PagedAttention test
â”œâ”€â”€ run_experiment.py         # Main experiment runner
â”œâ”€â”€ README.md                 # English documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
```

## ğŸ”¬ Experiment Details

### Test Scenarios

The experiment uses **identical inputs** for fair comparison:

1. **Short text**: `"Hello!"` (6 characters)
2. **Medium text**: `"How are you doing today? I hope everything is going well."` (57 characters)  
3. **Long text**: Repeated AI story prompt (680 characters)
4. **Short text**: `"Thanks!"` (7 characters)

### Key Metrics Measured

- ğŸ“Š **GPU Memory Usage**: Allocated vs Total memory
- â±ï¸ **Processing Time**: Per request and total time
- ğŸ”„ **Memory Patterns**: Allocation and deallocation cycles
- ğŸ“ˆ **Batch Efficiency**: Sequential vs parallel processing

### Expected Results

| Metric | Traditional Method | vLLM PagedAttention |
|--------|-------------------|-------------------|
| Memory Usage | High fluctuation, peaks | Stable and efficient |
| Processing Mode | Sequential | Batch parallel |
| Memory Allocation | Pre-allocate max length | On-demand paging |
| Throughput | Lower | Higher |

## ğŸ“Š Sample Output

```
ğŸ§ª PagedAttention Memory Analysis Experiment
============================================================

ğŸ”¬ Phase 1: Traditional Transformers Method
ğŸ’¡ Characteristics: Sequential processing, max-length allocation

=== Model Loading ===
ğŸ”¥ GPU_0: 1.2GB / 24.0GB (5.0%)

=== Processing Input 1 (6 characters) ===  
ğŸ”¥ GPU_0: 2.8GB / 24.0GB (11.7%)
â±ï¸ Generation time: 0.45s

ğŸ”¬ Phase 2: vLLM PagedAttention Method  
ğŸ’¡ Characteristics: Batch processing, paged allocation

=== Batch Processing (4 inputs) ===
ğŸ”¥ GPU_0: 2.1GB / 24.0GB (8.8%)
â±ï¸ Total time: 0.10s (0.025s per request)

ğŸ‰ Results: 78% memory efficiency improvement, 4.5x speedup!
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

**CUDA not available**
```bash
# Check CUDA installation
nvidia-smi
nvcc --version
```

**vLLM installation fails**
```bash
# Try installing from source
pip install git+https://github.com/vllm-project/vllm.git
```

**Missing Accelerate error**
```bash
pip install accelerate
```

**Out of memory**
```bash
# Use smaller model or reduce batch size
# Modify scripts to use "distilgpt2" instead of "gpt2"
```

## ğŸ”§ Customization

### Using Different Models

Edit the model name in both test scripts:
```python
# In test_traditional.py and test_vllm.py
model_name = "microsoft/DialoGPT-small"  # or any HF model
```

### Adding More Test Cases

Extend the test inputs in both scripts:
```python
test_inputs = [
    "Your custom short text",
    "Your custom medium length text...",
    "Your custom long text..." * 20,
    # Add more test cases
]
```

### Memory Monitoring Frequency

Adjust monitoring intervals in `memory_monitor.py`:
```python
time.sleep(0.5)  # Change monitoring frequency
```

## ğŸ“š Educational Value

This experiment teaches you:

1. **PagedAttention Principles**: How paged memory management improves efficiency
2. **LLM Inference Optimization**: Practical memory management techniques  
3. **Batch Processing Benefits**: Why batching improves throughput
4. **GPU Memory Patterns**: How different approaches use GPU memory
5. **Performance Analysis**: How to measure and compare ML system performance

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- ğŸ“Š Add visualization of memory usage patterns
- ğŸ”¬ Extend to larger models (Llama, GPT-3.5 scale)
- ğŸ“ˆ Add throughput vs latency analysis
- ğŸ› ï¸ Support for multi-GPU setups
- ğŸ“ Add Windows compatibility guide

---

â­ If this project helped you understand PagedAttention, please star the repository!

ğŸ”— **Related Projects**: [Awesome-LLM-Inference](link), [vLLM](https://github.com/vllm-project/vllm), [Transformers](https://github.com/huggingface/transformers)
