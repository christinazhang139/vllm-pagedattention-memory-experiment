# PagedAttention Experiment Detailed Step-by-Step Guide

## Step 1: Create Experimental Directory
### ðŸŽ¯ What this step does:
Create a dedicated folder for the experiment to avoid mixing with other projects

### ðŸ“‚ Specific Operations:
```bash
# Confirm current location
pwd
# Should show: /home/username/vllm-learning/experiments

# Create experiment folder
mkdir paged-attention-test
# ðŸ‘€ Purpose: Creates a new folder named "paged-attention-test"

cd paged-attention-test
# ðŸ‘€ Purpose: Enter the newly created experiment folder

# Create subdirectories
mkdir scripts logs results
# ðŸ‘€ Purpose:
#   â€¢ scripts - Store Python script files
#   â€¢ logs - Store runtime logs
#   â€¢ results - Store experimental results

ls -la
# ðŸ‘€ Purpose: View the created directory structure to confirm everything was created successfully
```

### ðŸ—‚ï¸ Final directory structure:
```
paged-attention-test/
â”œâ”€â”€ scripts/     (Python scripts)
â”œâ”€â”€ logs/        (Log files)
â””â”€â”€ results/     (Result data)
```

---

## Step 2: Create Virtual Environment
### ðŸŽ¯ What this step does:
Create an independent Python environment to avoid affecting existing Python packages on your system

### ðŸ Why we need a virtual environment:
- **Isolation**: Experimental packages won't affect system Python
- **Version control**: Can install specific package versions
- **Cleanliness**: Can delete directly after experiment without leaving traces

### ðŸ“‚ Specific Operations:
```bash
# Create Python virtual environment
python3 -m venv venv-paged-test
# ðŸ‘€ Purpose:
#   â€¢ Uses your Python 3.12 to create virtual environment
#   â€¢ Named "venv-paged-test"
#   â€¢ Creates a folder containing independent Python interpreter

# Activate virtual environment
source venv-paged-test/bin/activate
# ðŸ‘€ Purpose:
#   â€¢ Starts the virtual environment
#   â€¢ Packages installed afterward will only go into this environment
#   â€¢ Command line will show (venv-paged-test) prefix

# Confirm activation success
which python
# ðŸ‘€ Purpose: Shows current Python path, should point to virtual environment

python --version
# ðŸ‘€ Purpose: Confirms Python version is still 3.12.3
```

### ðŸ” Verification Effect:
After activation, command line should become:
```
(venv-paged-test) username@ubuntu:~/vllm-learning/experiments/paged-attention-test$
```

---

## Step 3: Install Missing Packages
### ðŸŽ¯ What this step does:
Install Python packages needed for the experiment in the virtual environment

### ðŸ“¦ Packages to install:
- **transformers**: Traditional large model inference library
- **vLLM**: High-efficiency inference library using PagedAttention
- **nvidia-ml-py3**: GPU monitoring tool
- **psutil**: System monitoring tool

### ðŸ“‚ Specific Operations:
```bash
# Upgrade pip (package manager)
pip install --upgrade pip
# ðŸ‘€ Purpose: Ensure pip is latest version to avoid package installation errors

# Install transformers
pip install transformers>=4.36.0
# ðŸ‘€ Purpose:
#   â€¢ Install HuggingFace's transformers library
#   â€¢ >=4.36.0 ensures Python 3.12 compatibility
#   â€¢ This is the core library for traditional method

# Install vLLM
pip install vllm
# ðŸ‘€ Purpose:
#   â€¢ Install vLLM library containing PagedAttention implementation
#   â€¢ This is the new method we want to compare
#   â€¢ May take several minutes to download and compile

# Install accelerate (required for transformers)
pip install accelerate
# ðŸ‘€ Purpose: Required dependency for transformers device_map functionality

# Install monitoring tools
pip install nvidia-ml-py3 psutil
# ðŸ‘€ Purpose:
#   â€¢ nvidia-ml-py3: Direct access to GPU information
#   â€¢ psutil: Monitor system resource usage

# Install other tools
pip install matplotlib pandas tqdm
# ðŸ‘€ Purpose: Data analysis and visualization tools (optional)
```

---

## Step 4: Verify Installation
### ðŸŽ¯ What this step does:
Confirm all packages are correctly installed and environment setup is successful

### ðŸ“‚ Specific Operations:
```bash
# Check PyTorch
python -c "import torch; print('âœ… PyTorch:', torch.__version__)"
# ðŸ‘€ Purpose: Confirm PyTorch can import normally, display version number

# Check Transformers
python -c "import transformers; print('âœ… Transformers:', transformers.__version__)"
# ðŸ‘€ Purpose: Confirm transformers installation success

# Check vLLM
python -c "import vllm; print('âœ… vLLM:', vllm.__version__)"
# ðŸ‘€ Purpose: Confirm vLLM installation success

# Check CUDA
python -c "import torch; print('âœ… CUDA available:', torch.cuda.is_available()); print('âœ… GPU count:', torch.cuda.device_count())"
# ðŸ‘€ Purpose: Confirm GPU can be recognized by PyTorch
```

### ðŸŽ‰ Successful output should look like:
```
âœ… PyTorch: 2.7.1
âœ… Transformers: 4.37.2
âœ… vLLM: 0.3.0
âœ… CUDA available: True
âœ… GPU count: 1
```

---

## Step 5: Create Memory Monitoring Script
### ðŸŽ¯ What this step does:
Write a Python script to monitor GPU memory usage

### ðŸ“ Where to run:
```bash
# Ensure you're in this directory:
pwd
# Should show: /home/username/vllm-learning/experiments/paged-attention-test

# Ensure virtual environment is activated:
source venv-paged-test/bin/activate
# Command line should show: (venv-paged-test)
```

### ðŸ” Why we need monitoring:
- **Comparison key**: See memory usage differences between two methods
- **Real-time observation**: Understand memory usage change patterns
- **Quantitative analysis**: Use data to prove PagedAttention advantages

### ðŸ“‚ Create script:
```bash
cat > scripts/memory_monitor.py << 'EOF'
import torch
import time
from datetime import datetime

class MemoryMonitor:
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        if self.gpu_available:
            try:
                import nvidia_ml_py3 as nvml
                nvml.nvmlInit()
                self.nvml = nvml
                self.device_count = nvml.nvmlDeviceGetCount()
                self.nvml_available = True
                print("âœ… Using NVML for GPU monitoring")
            except ImportError:
                print("âš ï¸  NVML unavailable, using PyTorch for GPU monitoring")
                self.nvml_available = False
        else:
            print("âŒ GPU unavailable")
    
    def get_gpu_memory_torch(self):
        """Get GPU memory using torch"""
        if not self.gpu_available:
            return {"error": "No GPU available"}
        
        memory_info = {}
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            
            memory_info[f"GPU_{i}"] = {
                "total": total,
                "allocated": allocated,
                "cached": cached,
                "free": total - cached,
                "utilization": (allocated / total) * 100 if total > 0 else 0
            }
        return memory_info
    
    def get_gpu_memory_nvml(self):
        """Get GPU memory using NVML"""
        if not self.gpu_available or not self.nvml_available:
            return self.get_gpu_memory_torch()
            
        memory_info = {}
        for i in range(self.device_count):
            handle = self.nvml.nvmlDeviceGetHandleByIndex(i)
            memory = self.nvml.nvmlDeviceGetMemoryInfo(handle)
            
            memory_info[f"GPU_{i}"] = {
                "total": memory.total / 1024**3,
                "used": memory.used / 1024**3,
                "free": memory.free / 1024**3,
                "utilization": (memory.used / memory.total) * 100
            }
        return memory_info
    
    def get_gpu_memory(self):
        """Get GPU memory usage"""
        if self.nvml_available:
            return self.get_gpu_memory_nvml()
        else:
            return self.get_gpu_memory_torch()
    
    def print_memory_status(self, label=""):
        """Print memory status"""
        print(f"\n=== {label} ===")
        print(f"Time: {datetime.now().strftime('%H:%M:%S')}")
        
        # GPU memory
        gpu_memory = self.get_gpu_memory()
        if "error" not in gpu_memory:
            for gpu_id, mem in gpu_memory.items():
                if "used" in mem:
                    print(f"ðŸ”¥ {gpu_id}: {mem['used']:.2f}GB / {mem['total']:.2f}GB ({mem['utilization']:.1f}%)")
                else:
                    print(f"ðŸ”¥ {gpu_id}: {mem['allocated']:.2f}GB allocated / {mem['total']:.2f}GB total")
        else:
            print("âŒ Failed to get GPU information")
        
        print("-" * 50)

if __name__ == "__main__":
    monitor = MemoryMonitor()
    monitor.print_memory_status("System Status Check")
    
    # Test GPU availability
    if torch.cuda.is_available():
        print(f"âœ… CUDA available, device count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"ðŸ“Š GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âŒ CUDA unavailable")
EOF
```

### ðŸŽ¯ This script enables us to:
- Compare memory usage before and after model loading
- Observe memory changes when processing different requests
- Quantify memory efficiency differences between two methods

---

## Step 6: Test Monitoring Script
### ðŸŽ¯ What this step does:
Run the just-created monitoring script to ensure it works properly

### ðŸ“ Where to run:
```bash
# Ensure in project root directory
pwd
# Should show: /home/username/vllm-learning/experiments/paged-attention-test

# Ensure virtual environment is activated
source venv-paged-test/bin/activate

# Run script from project root directory
python scripts/memory_monitor.py
```
**Note**: Script file is in `scripts/` folder, but we run it from project root directory

### ðŸŽ‰ Normal output should look like:
```
=== System Status Check ===
Time: 20:30:15
ðŸ”¥ GPU_0: 0.56GB / 24.00GB (2.3%)
--------------------------------------------------
âœ… CUDA available, device count: 1
ðŸ“Š GPU 0: NVIDIA GeForce RTX 4090
```

---

## Step 7: Create Traditional Method Test Script
### ðŸŽ¯ What this step does:
Write a script to test traditional Transformers library memory usage patterns

### ðŸ” Traditional method characteristics:
- **Sequential processing**: Can only process one request at a time
- **Fixed allocation**: Allocates memory by maximum possible length
- **Memory waste**: Short texts also occupy memory space for long texts

### ðŸ“‚ Create script:
```bash
cat > scripts/test_traditional.py << 'EOF'
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from memory_monitor import MemoryMonitor

def test_traditional_method():
    """Test traditional Transformers method"""
    monitor = MemoryMonitor()
    
    print("ðŸ”„ Starting traditional method test...")
    monitor.print_memory_status("Initial state")
    
    # Use GPT-2 model for testing
    model_name = "gpt2"
    
    print("ðŸ“¥ Loading traditional Transformers model...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        # Manually move to GPU
        if torch.cuda.is_available():
            model = model.to("cuda")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        monitor.print_memory_status("Model loading complete")
        
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        return
    
    # Prepare test data - different length texts
    test_inputs = [
        "Hello!",  # Short text
        "How are you doing today? I hope everything is going well.",  # Medium length
        "Tell me a story about artificial intelligence and machine learning. " * 10,  # Long text
        "Thanks!"  # Short text
    ]
    
    print(f"ðŸ“ Processing {len(test_inputs)} inputs of different lengths...")
    
    total_time = 0
    for i, input_text in enumerate(test_inputs):
        print(f"\nðŸ“„ Processing input {i+1}: length {len(input_text)} characters")
        
        try:
            # Encode input
            inputs = tokenizer(
                input_text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            
            # Move to GPU
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # Generate text
            with torch.no_grad():
                start_time = time.time()
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )
                generation_time = time.time() - start_time
                total_time += generation_time
            
            # Decode output
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"â±ï¸  Generation time: {generation_time:.2f}s")
            print(f"ðŸ“Š Output length: {len(generated_text)} characters")
            
            monitor.print_memory_status(f"Completed input {i+1}")
            
            # Clean GPU cache
            torch.cuda.empty_cache()
            time.sleep(1)  # Wait for memory release
                
        except Exception as e:
            print(f"âŒ Error processing input {i+1}: {e}")
    
    print(f"\nâœ… Traditional method test complete!")
    print(f"ðŸ•’ Total time: {total_time:.2f}s")
    print(f"ðŸ“ˆ Average per request: {total_time/len(test_inputs):.2f}s")
    print(f"ðŸ’¡ Key observation: Each request needs separate processing, memory usage fluctuates")

if __name__ == "__main__":
    test_traditional_method()
EOF
```

### ðŸ”§ If encountering accelerate-related errors:
```bash
# Install missing dependency
pip install accelerate

# Re-run test
python scripts/test_traditional.py
```

### ðŸŽ¯ Through this test we can observe:
- How memory is allocated each time processing
- Whether memory usage differs for different text lengths
- Traditional method's memory usage patterns

---

## Step 8: Create vLLM Test Script
### ðŸŽ¯ What this step does:
Write a script to test vLLM library's PagedAttention memory usage patterns

### ðŸ” PagedAttention characteristics:
- **Batch processing**: Can process multiple requests simultaneously
- **Paged allocation**: Allocates memory pages based on actual needs
- **High efficiency**: Avoids memory waste and fragmentation

### ðŸ“‚ Create script:
```bash
cat > scripts/test_vllm.py << 'EOF'
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import time
from memory_monitor import MemoryMonitor

def test_vllm_method():
    """Test vLLM PagedAttention method"""
    monitor = MemoryMonitor()
    
    print("ðŸ”„ Starting vLLM method test...")
    monitor.print_memory_status("Initial state")
    
    try:
        from vllm import LLM, SamplingParams
        print("âœ… vLLM import successful")
    except ImportError as e:
        print(f"âŒ vLLM import failed: {e}")
        print("ðŸ’¡ Please ensure vLLM is installed: pip install vllm")
        return
    
    # Load model
    print("ðŸ“¥ Loading vLLM model...")
    try:
        llm = LLM(
            model="gpt2",
            trust_remote_code=True,
            max_model_len=512,
            gpu_memory_utilization=0.8  # Use 80% of GPU memory
        )
        monitor.print_memory_status("vLLM model loading complete")
    except Exception as e:
        print(f"âŒ vLLM model loading failed: {e}")
        return
    
    # Set sampling parameters
    sampling_params = SamplingParams(
        temperature=0.7,
        max_tokens=50,
        top_p=0.95
    )
    
    # Prepare test data (same as traditional method)
    test_inputs = [
        "Hello!",  # Short text
        "How are you doing today? I hope everything is going well.",  # Medium length  
        "Tell me a story about artificial intelligence and machine learning. " * 10,  # Long text
        "Thanks!"  # Short text
    ]
    
    print(f"ðŸ“ Processing {len(test_inputs)} inputs of different lengths...")
    print("ðŸš€ vLLM advantage: Batch processing all requests!")
    
    try:
        # Batch process all inputs - this is vLLM's core advantage
        start_time = time.time()
        outputs = llm.generate(test_inputs, sampling_params)
        total_time = time.time() - start_time
        
        monitor.print_memory_status("Batch generation complete")
        
        # Display results
        for i, output in enumerate(outputs):
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"\nðŸ“„ Input {i+1}: '{prompt[:50]}...'")
            print(f"ðŸ“Š Original length: {len(prompt)} characters")
            print(f"ðŸ“Š Generated length: {len(generated_text)} characters")
        
        print(f"\nâœ… vLLM method test complete!")
        print(f"ðŸ•’ Total generation time: {total_time:.2f}s")
        print(f"ðŸ“ˆ Average per request: {total_time/len(test_inputs):.2f}s")
        print(f"ðŸ”¥ Core advantage: Batch processing, more stable memory usage, supports PagedAttention!")
        
    except Exception as e:
        print(f"âŒ vLLM generation process error: {e}")

if __name__ == "__main__":
    test_vllm_method()
EOF
```

### ðŸŽ¯ Through this test we can observe:
- Batch processing efficiency advantages
- Memory usage stability
- PagedAttention's actual effects

---

## Step 9: Create Run Script
### ðŸŽ¯ What this step does:
Write a master control script to automatically run the entire experimental process

### ðŸ“‚ Create script:
```bash
cat > run_experiment.py << 'EOF'
#!/usr/bin/env python3
import subprocess
import sys
import os
import time

def check_environment():
    """Check experimental environment"""
    print("ðŸ” Checking experimental environment...")
    
    # Check Python version
    print(f"ðŸ Python version: {sys.version}")
    
    # Check virtual environment
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âœ… Running in virtual environment")
    else:
        print("âš ï¸  Recommend running in virtual environment")
    
    # Check necessary modules
    try:
        import torch
        print(f"âœ… PyTorch version: {torch.__version__}")
        print(f"âœ… CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ðŸ“Š GPU count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    except ImportError:
        print("âŒ PyTorch not installed")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers version: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformers not installed")
        return False
    
    try:
        import vllm
        print(f"âœ… vLLM version: {vllm.__version__}")
    except ImportError:
        print("âŒ vLLM not installed")
        return False
    
    return True

def run_experiment():
    """Run comparison experiment"""
    print("\n" + "=" * 60)
    print("ðŸ§ª PagedAttention Memory Allocation Comparison Experiment")
    print("ðŸŽ¯ Goal: Understand how PagedAttention improves memory utilization")
    print("=" * 60)
    
    if not check_environment():
        print("âŒ Environment check failed, please install necessary dependencies")
        return
    
    script_dir = "scripts"
    
    # Test traditional method
    print("\n" + "="*50)
    print("ðŸ”¬ Phase 1: Traditional Transformers Method Test")
    print("ðŸ’¡ Characteristics: Sequential processing, max-length memory allocation")
    print("="*50)
    
    try:
        print("â³ Running traditional method test...")
        result = subprocess.run([sys.executable, f"{script_dir}/test_traditional.py"], 
                              cwd=os.getcwd())
        if result.returncode == 0:
            print("\nâœ… Traditional method test complete")
        else:
            print(f"\nâŒ Traditional method test failed, return code: {result.returncode}")
    except Exception as e:
        print(f"âŒ Traditional method test error: {e}")
    
    # Wait for user confirmation
    print("\n" + "="*50)
    input("â¸ï¸  Press Enter to continue with vLLM PagedAttention method test...")
    
    # Test vLLM method
    print("\n" + "="*50)
    print("ðŸ”¬ Phase 2: vLLM PagedAttention Method Test")
    print("ðŸ’¡ Characteristics: Batch processing, paged KV-Cache management, on-demand memory allocation")
    print("="*50)
    
    try:
        print("â³ Running vLLM method test...")
        result = subprocess.run([sys.executable, f"{script_dir}/test_vllm.py"], 
                              cwd=os.getcwd())
        if result.returncode == 0:
            print("\nâœ… vLLM method test complete")
        else:
            print(f"\nâŒ vLLM method test failed, return code: {result.returncode}")
    except Exception as e:
        print(f"âŒ vLLM method test error: {e}")
    
    # Experiment summary
    print("\n" + "=" * 60)
    print("ðŸŽ‰ Experiment Complete! PagedAttention Principle Analysis")
    print("=" * 60)
    print("\nðŸ” Key Observations:")
    print("1. ðŸ’¾ GPU Memory Usage Patterns:")
    print("   â€¢ Traditional method: Each sequence allocated by max length, memory waste exists")
    print("   â€¢ PagedAttention: Pages allocated based on actual needs, higher memory utilization")
    print("\n2. ðŸ“Š Memory Usage Stability:")
    print("   â€¢ Traditional method: Memory usage fluctuates greatly with input length")
    print("   â€¢ PagedAttention: More stable memory usage, better predictability")
    print("\n3. âš¡ Processing Efficiency:")
    print("   â€¢ Traditional method: Sequential request processing")
    print("   â€¢ PagedAttention: Supports efficient batch processing")
    print("\nðŸ’¡ PagedAttention Core Innovations:")
    print("ðŸ”¹ Splits KV-Cache into fixed-size pages")
    print("ðŸ”¹ Allocates pages on-demand, avoiding pre-allocation of large contiguous memory blocks")
    print("ðŸ”¹ Supports dynamic memory management and better memory fragmentation handling")
    print("ðŸ”¹ Enables batch processing of variable-length sequences")
    print("\nðŸŽ¯ Practical Significance:")
    print("ðŸ“ˆ Supports larger batch sizes on same hardware")
    print("ðŸ’° Reduces hardware costs for inference services")
    print("ðŸš€ Improves model service throughput")

if __name__ == "__main__":
    run_experiment()
EOF

# Give scripts execution permissions
chmod +x run_experiment.py
chmod +x scripts/*.py
```

### ðŸŽ¯ Value of this script:
- **Automation**: One-click run of all tests
- **User-friendly**: Clear prompts and explanations
- **Educational**: Explains the principles behind each phenomenon

---

## Step 10: Run Experiment
### ðŸŽ¯ What this step does:
Execute the complete comparison experiment and observe differences between the two methods

### ðŸ“ Where to run:
```bash
# 1. Ensure in correct directory
pwd
# Must show: /home/username/vllm-learning/experiments/paged-attention-test

# 2. Ensure virtual environment is activated
source venv-paged-test/bin/activate
# Command line should show: (venv-paged-test)

# 3. Run main experiment script
python run_experiment.py
```

### ðŸ—‚ï¸ Important directory structure:
```
/home/username/vllm-learning/experiments/paged-attention-test/  â† You run here
â”œâ”€â”€ venv-paged-test/           â† Virtual environment
â”œâ”€â”€ scripts/                   â† Script files location
â”‚   â”œâ”€â”€ memory_monitor.py      â† Monitoring script
â”‚   â”œâ”€â”€ test_traditional.py    â† Traditional method test
â”‚   â””â”€â”€ test_vllm.py          â† vLLM method test
â”œâ”€â”€ logs/                      â† Log output
â”œâ”€â”€ results/                   â† Result files
â””â”€â”€ run_experiment.py          â† Main run script (here)
```

### ðŸ”„ Script call relationship:
```
You run: python run_experiment.py  (in root directory)
   â†“
Auto calls: python scripts/test_traditional.py
   â†“  
Auto calls: python scripts/test_vllm.py
```

### ðŸŽ¯ Key differences you will observe:

#### Traditional Method Characteristics:
- ðŸ“Š Memory usage: Fluctuates with input length
- â±ï¸ Processing mode: Sequential request processing
- ðŸ’¾ Memory allocation: Pre-allocate by maximum length

#### PagedAttention Characteristics:
- ðŸ“Š Memory usage: More stable and efficient
- â±ï¸ Processing mode: Batch processing
- ðŸ’¾ Memory allocation: On-demand paged allocation

### ðŸŽ“ Educational value of the experiment:
Through this experiment, you will deeply understand:
1. **How PagedAttention works**
2. **Why it can improve memory utilization**
3. **Its advantages in practical applications**

---

## Summary: Core Purpose of Each Step

| Step | Main Purpose | Key Output |
|------|--------------|------------|
| 1-2 | Environment preparation | Independent experimental environment |
| 3-4 | Dependency installation | Complete software stack |
| 5-6 | Monitoring tools | Memory usage observation capability |
| 7-8 | Test scripts | Comparison tests for both methods |
| 9-10 | Experiment execution | Intuitive understanding of PagedAttention principles |

Each step serves the ultimate goal: **Through practical comparison, gain deep understanding of PagedAttention's working principles and advantages**.
